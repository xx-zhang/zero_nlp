{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from thuglm.modeling_chatglm import ChatGLMForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at test003//checkpoint-500 were not used when initializing ChatGLMForConditionalGeneration: ['transformer.layers.20.attention.query_key_value.lora_A.weight', 'transformer.layers.7.attention.query_key_value.lora_B.weight', 'transformer.layers.8.attention.query_key_value.lora_A.weight', 'transformer.layers.23.attention.query_key_value.lora_B.weight', 'transformer.layers.23.attention.query_key_value.lora_A.weight', 'transformer.layers.17.attention.query_key_value.lora_B.weight', 'transformer.layers.26.attention.query_key_value.lora_A.weight', 'transformer.layers.18.attention.query_key_value.lora_B.weight', 'transformer.layers.5.attention.query_key_value.lora_B.weight', 'transformer.layers.27.attention.query_key_value.lora_A.weight', 'transformer.layers.20.attention.query_key_value.lora_B.weight', 'transformer.layers.24.attention.query_key_value.lora_B.weight', 'transformer.layers.8.attention.query_key_value.lora_B.weight', 'transformer.layers.13.attention.query_key_value.lora_A.weight', 'transformer.layers.21.attention.query_key_value.lora_B.weight', 'transformer.layers.21.attention.query_key_value.lora_A.weight', 'transformer.layers.0.attention.query_key_value.lora_A.weight', 'transformer.layers.15.attention.query_key_value.lora_A.weight', 'transformer.layers.1.attention.query_key_value.lora_A.weight', 'transformer.layers.13.attention.query_key_value.lora_B.weight', 'transformer.layers.25.attention.query_key_value.lora_A.weight', 'transformer.layers.9.attention.query_key_value.lora_A.weight', 'transformer.layers.10.attention.query_key_value.lora_A.weight', 'transformer.layers.10.attention.query_key_value.lora_B.weight', 'transformer.layers.4.attention.query_key_value.lora_B.weight', 'transformer.layers.16.attention.query_key_value.lora_B.weight', 'transformer.layers.12.attention.query_key_value.lora_A.weight', 'transformer.layers.25.attention.query_key_value.lora_B.weight', 'transformer.layers.19.attention.query_key_value.lora_B.weight', 'transformer.layers.5.attention.query_key_value.lora_A.weight', 'transformer.layers.7.attention.query_key_value.lora_A.weight', 'transformer.layers.26.attention.query_key_value.lora_B.weight', 'transformer.layers.27.attention.query_key_value.lora_B.weight', 'transformer.layers.2.attention.query_key_value.lora_B.weight', 'transformer.layers.17.attention.query_key_value.lora_A.weight', 'transformer.layers.3.attention.query_key_value.lora_A.weight', 'transformer.layers.6.attention.query_key_value.lora_A.weight', 'transformer.layers.0.attention.query_key_value.lora_B.weight', 'transformer.layers.12.attention.query_key_value.lora_B.weight', 'transformer.layers.1.attention.query_key_value.lora_B.weight', 'transformer.layers.4.attention.query_key_value.lora_A.weight', 'transformer.layers.22.attention.query_key_value.lora_A.weight', 'transformer.layers.22.attention.query_key_value.lora_B.weight', 'transformer.layers.14.attention.query_key_value.lora_A.weight', 'transformer.layers.16.attention.query_key_value.lora_A.weight', 'transformer.layers.9.attention.query_key_value.lora_B.weight', 'transformer.layers.3.attention.query_key_value.lora_B.weight', 'transformer.layers.19.attention.query_key_value.lora_A.weight', 'transformer.layers.14.attention.query_key_value.lora_B.weight', 'transformer.layers.11.attention.query_key_value.lora_B.weight', 'transformer.layers.18.attention.query_key_value.lora_A.weight', 'transformer.layers.11.attention.query_key_value.lora_A.weight', 'transformer.layers.2.attention.query_key_value.lora_A.weight', 'transformer.layers.15.attention.query_key_value.lora_B.weight', 'transformer.layers.6.attention.query_key_value.lora_B.weight', 'transformer.layers.24.attention.query_key_value.lora_A.weight']\n",
      "- This IS expected if you are initializing ChatGLMForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ChatGLMForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ChatGLMForConditionalGeneration.from_pretrained(\"test003//checkpoint-500\").cuda() #\n",
    "\n",
    "# model = ChatGLMForConditionalGeneration.from_pretrained(\"thuglm\").half().cuda() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0086,  0.0008, -0.0302,  ...,  0.0129,  0.0072,  0.0076],\n",
       "        [-0.0206,  0.0022,  0.0035,  ...,  0.0080, -0.0023,  0.0106],\n",
       "        [ 0.0200,  0.0214,  0.0087,  ..., -0.0035, -0.0123, -0.0215],\n",
       "        ...,\n",
       "        [-0.0159, -0.0188, -0.0100,  ..., -0.0026,  0.0239, -0.0218],\n",
       "        [-0.0130,  0.0199,  0.0193,  ...,  0.0080, -0.0138, -0.0008],\n",
       "        [ 0.0008, -0.0070,  0.0097,  ..., -0.0036, -0.0054, -0.0129]],\n",
       "       device='cuda:0', dtype=torch.float16, requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.layers[0].attention.dense.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"thuglm\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 20005, 128293,  20031,  84035,  86284,  85455, 121020,  83978, 106122,\n",
      "          85015,  85244,  31943,  87077, 150001, 150004]], device='cuda:0'), 'max_length': 2048, 'num_beams': 1, 'do_sample': True, 'top_p': 0.7, 'temperature': 0.95, 'eos_token_id': 150005}\n",
      "作为一个AI模型,我没有真正的身份或自我意识,我只是由算法和数据组成的程序。我被训练来回答你的问题和提供帮助,但我不能告诉你我是由哪个程序员训练的。\n"
     ]
    }
   ],
   "source": [
    "with torch.autocast(\"cuda\"):\n",
    "    res, history = model.chat(tokenizer=tokenizer, query=\"说一个对联\")\n",
    "        # res = model.forward(input_ids=all_input.get('input_ids').cuda())\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[108915,  85455, 121020,  83978, 106122,  85015,  85244,  31943,  87077,\n",
      "          83964,  20031, 150001, 150004]], device='cuda:0'), 'max_length': 2048, 'num_beams': 1, 'do_sample': True, 'top_p': 0.7, 'temperature': 0.95, 'eos_token_id': 150005}\n",
      "我不是良睦路程序员训练的AI模型,我是一个由清华大学 KEG 实验室和智谱AI训练的大型语言模型。我的任务是针对用户的问题和要求提供适当的答复和支持。我无法进行编程,也不能进行任何实际的工作。我只是一个机器程序,旨在提供信息和回答问题。\n"
     ]
    }
   ],
   "source": [
    "with torch.autocast(\"cuda\"):\n",
    "    res, history = model.chat(tokenizer=tokenizer, query=\"庄子和老子什么关系\")\n",
    "    # res = model.forward(input_ids=all_input.get('input_ids').cuda())\n",
    "\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autocast(\"cuda\"):\n",
    "    res, history = model.chat(tokenizer=tokenizer, query=\"介绍下防火墙\")\n",
    "    # res = model.forward(input_ids=all_input.get('input_ids').cuda())\n",
    "\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 20005,  85522,  84912,  91256,  85185, 150001, 150004]],\n",
      "       device='cuda:0'), 'max_length': 2048, 'num_beams': 1, 'do_sample': True, 'top_p': 0.7, 'temperature': 0.95, 'eos_token_id': 150005}\n",
      "密室杀人案是指一个人或一群人在一间密室内,因为某种原因(通常是感情或仇恨)而互相残杀的案件。这种案件通常会引起广泛关注和讨论,因为它们往往具有独特的情节和复杂性。\n",
      "\n",
      "密室杀人案通常会涉及以下几个方面:\n",
      "\n",
      "1. 密室:通常是一间小房间或房间,可以用于睡觉、工作或其他非暴力活动,但不允许其他人进入。\n",
      "\n",
      "2. 动机:通常涉及个人感情或仇恨,导致人们互相残杀。\n",
      "\n",
      "3. 参与者:参与者可以是同一人或不同人,但通常是多人。\n",
      "\n",
      "4. 武器:密室杀人案中通常会使用武器,例如刀、枪等。\n",
      "\n",
      "5. 证据:案件通常需要进行调查和搜寻,以找到足够的证据来证明谁是罪犯。\n",
      "\n",
      "密室杀人案是一种非常危险和复杂的犯罪行为,需要我们保持警惕并加强安全意识。\n"
     ]
    }
   ],
   "source": [
    "with torch.autocast(\"cuda\"):\n",
    "    res, history = model.chat(tokenizer=tokenizer, query=\"密室杀人案\")\n",
    "    # res = model.forward(input_ids=all_input.get('input_ids').cuda())\n",
    "\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
